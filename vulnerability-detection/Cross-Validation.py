import pickle
import numpy as np
import torch
import random
import time
import warnings
from torch.optim import lr_scheduler
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from torch_geometric.loader import DataLoader
from torch.utils.data import WeightedRandomSampler
from collections import Counter
from sklearn.model_selection import StratifiedKFold
from torch_geometric.nn import GCNConv, ChebConv, global_mean_pool
from torch.nn import Linear
import torch.nn.functional as F
from scipy.stats import ttest_rel
import pandas as pd

warnings.filterwarnings("ignore")


def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    np.random.seed(seed)
    random.seed(seed)


setup_seed(42)


def load_and_scale_data(file_path):
    with open(file_path, 'rb') as f:
        dataset = pickle.load(f)
    mean = torch.mean(torch.cat([data['x'] for data in dataset]), dim=0)
    std = torch.std(torch.cat([data['x'] for data in dataset]), dim=0)
    for data in dataset:
        data['x'] = (data['x'] - mean) / std
    return dataset


def perturb_node_features(x, noise_level=0.01):
    if isinstance(x, np.ndarray):
        x = torch.tensor(x)
    x = x + noise_level * torch.randn_like(x)
    return x


def mask_node_features(x, mask_rate=0.1):
    num_nodes, num_features = x.size()
    mask = np.random.binomial(1, mask_rate, (num_nodes, num_features))
    mask = torch.FloatTensor(mask).to(x.device)
    return x * (1 - mask)


class GCN(torch.nn.Module):
    def __init__(self, hidden_channels):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(128, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, 32)
        self.conv3 = ChebConv(128, hidden_channels, K=2)
        self.conv4 = ChebConv(hidden_channels, 32, K=2)
        self.lin = Linear(32, 2)
        self.dropout_rate = 0.5

    def forward(self, x, edge_index, batch):
        x1 = perturb_node_features(x, noise_level=0.01)
        x1 = self.conv1(x1, edge_index).relu()
        x1 = F.dropout(x1, p=self.dropout_rate, training=self.training)
        x1 = self.conv2(x1, edge_index)

        x2 = mask_node_features(x, mask_rate=0.1)
        x2 = self.conv3(x2, edge_index).relu()
        x2 = F.dropout(x2, p=self.dropout_rate, training=self.training)
        x2 = self.conv4(x2, edge_index)

        x = x1 + x2
        x = F.dropout(x, p=self.dropout_rate, training=self.training)
        x = global_mean_pool(x, batch)
        return self.lin(x)


def train(model, loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for data in loader:
        data = data.to(device)
        out = model(data.x, data.edge_index, data.batch)
        loss = criterion(out, data.y)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        total_loss += loss.item()
    return total_loss / len(loader)


def test(model, loader, criterion, device):
    model.eval()
    total_loss, total_acc, total_f1, total_precision, total_recall = 0, 0, 0, 0, 0
    with torch.no_grad():
        for data in loader:
            data = data.to(device)
            out = model(data.x, data.edge_index, data.batch)
            loss = criterion(out, data.y)
            pred = out.argmax(dim=1)
            total_loss += loss.item()
            total_acc += accuracy_score(data.y.cpu(), pred.cpu())
            total_f1 += f1_score(data.y.cpu(), pred.cpu(), average='weighted')
            total_precision += precision_score(data.y.cpu(), pred.cpu(), average='weighted')
            total_recall += recall_score(data.y.cpu(), pred.cpu(), average='weighted')
    n = len(loader)
    return {
        'loss': total_loss / n,
        'acc': total_acc / n,
        'f1': total_f1 / n,
        'precision': total_precision / n,
        'recall': total_recall / n
    }


def run_k_fold(dataset, k=10, epochs=100):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
    fold_results = []
    labels = [data.y.item() for data in dataset]

    for fold, (train_idx, test_idx) in enumerate(skf.split(dataset, labels)):
        print(f"Fold {fold + 1}")
        train_dataset = [dataset[i] for i in train_idx]
        test_dataset = [dataset[i] for i in test_idx]

        labels = [data.y.item() for data in train_dataset]
        label_counts = Counter(labels)
        weight = np.array([1 / label_counts[0], 1 / label_counts[1]])
        samples_weight = [weight[label] for label in labels]
        sampler = WeightedRandomSampler(torch.tensor(samples_weight, dtype=torch.float32), len(train_dataset),
                                        replacement=True)

        train_loader = DataLoader(train_dataset, batch_size=256, sampler=sampler)
        test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)

        model = GCN(hidden_channels=64).to(device)
        optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
        criterion = torch.nn.CrossEntropyLoss()
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=200, threshold=1e-8,
                                                   min_lr=1e-8)

        best_test_metrics = None

        with open(f'fold_{fold + 1}_results.txt', 'w') as f:
            f.write("Epoch,Train Loss,Test Loss,Test Acc,Test F1,Test Precision,Test Recall\n")
            for epoch in range(1, epochs + 1):
                train_loss = train(model, train_loader, optimizer, criterion, device)
                test_metrics = test(model, test_loader, criterion, device)
                scheduler.step(test_metrics['loss'])

                if best_test_metrics is None or test_metrics['f1'] > best_test_metrics['f1']:
                    best_test_metrics = test_metrics

                f.write(
                    f"{epoch},{train_loss:.4f},{test_metrics['loss']:.4f},{test_metrics['acc']:.4f},{test_metrics['f1']:.4f},{test_metrics['precision']:.4f},{test_metrics['recall']:.4f}\n")

        print(f"Best results for fold {fold + 1}: {best_test_metrics}")
        fold_results.append(best_test_metrics)

    return fold_results


def run_multiple_random_selections(dataset, num_selections=5, epochs=200):
    all_results = []
    for selection in range(num_selections):
        print(f"Random selection {selection + 1}")
        fold_results = run_k_fold(dataset, k=10, epochs=epochs)
        all_results.append([fold['f1'] for fold in fold_results])

    # t-test
    baseline = all_results[0]
    p_values = []
    for i in range(1, num_selections):
        _, p_value = ttest_rel(baseline, all_results[i])
        p_values.append(p_value)

    print("p-values:", p_values)

    means = [np.mean(results) for results in all_results]
    stds = [np.std(results) for results in all_results]

    data = {
        'Selection': range(1, len(all_results) + 1),
        'Mean F1': means,
        'Std F1': stds,
        'p-value': [1.0] + p_values
    }
    df = pd.DataFrame(data)

    print(df)
    df.to_csv('results_table.csv', index=False)

    return all_results


if __name__ == '__main__':
    start_time = time.time()
    dataset = load_and_scale_data('dataset2.pkl')
    all_results = run_multiple_random_selections(dataset, num_selections=5, epochs=200)
    print(f"Total Runtime: {time.time() - start_time:.2f} seconds")
    
